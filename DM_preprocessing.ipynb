{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>White House news conference at 5:00 P.M. Easte...</td>\n",
       "      <td>2020-03-30 20:50:35+00:00</td>\n",
       "      <td>14441</td>\n",
       "      <td>1244728753617620992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>https://t.co/2hKJkP5Z6N</td>\n",
       "      <td>2020-03-30 17:46:15+00:00</td>\n",
       "      <td>15520</td>\n",
       "      <td>1244682364284014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>On #NationalDoctorsDay, we recognize the remar...</td>\n",
       "      <td>2020-03-30 17:11:59+00:00</td>\n",
       "      <td>19753</td>\n",
       "      <td>1244673740866191360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>https://t.co/nzWJ8ViwbZ</td>\n",
       "      <td>2020-03-30 17:05:33+00:00</td>\n",
       "      <td>39114</td>\n",
       "      <td>1244672122414338048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Nancy Pelosi and the Democrats delayed the Wor...</td>\n",
       "      <td>2020-03-30 11:17:10+00:00</td>\n",
       "      <td>43360</td>\n",
       "      <td>1244584449309892608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text  \\\n",
       "0  Twitter for iPhone  White House news conference at 5:00 P.M. Easte...   \n",
       "1  Twitter for iPhone                            https://t.co/2hKJkP5Z6N   \n",
       "2  Twitter for iPhone  On #NationalDoctorsDay, we recognize the remar...   \n",
       "3  Twitter for iPhone                            https://t.co/nzWJ8ViwbZ   \n",
       "4  Twitter for iPhone  Nancy Pelosi and the Democrats delayed the Wor...   \n",
       "\n",
       "                 created_at  retweet_count               id_str  \n",
       "0 2020-03-30 20:50:35+00:00          14441  1244728753617620992  \n",
       "1 2020-03-30 17:46:15+00:00          15520  1244682364284014592  \n",
       "2 2020-03-30 17:11:59+00:00          19753  1244673740866191360  \n",
       "3 2020-03-30 17:05:33+00:00          39114  1244672122414338048  \n",
       "4 2020-03-30 11:17:10+00:00          43360  1244584449309892608  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data loading\n",
    "path = '/Users/maida/Downloads/'\n",
    "data = pd.read_json(path + 'data.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "\n",
    "\"\"\"\n",
    "This is preprocessing section. Following steps were taken:\n",
    "\n",
    "    1. punctuation removal with '!' exception\n",
    "    2. numbers removal\n",
    "    3. url removals\n",
    "    4. normalization to lowercase with exception for isupper()\n",
    "    5. string to tokens\n",
    "    6. stopwords removal\n",
    "    7. stemming\n",
    "    8. normalization of elongated words\n",
    "\"\"\"\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "data.dtypes\n",
    "\n",
    "# changing float64 to string\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        if punctuation != '!': # leave exclamation mark\n",
    "            text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "# remove numbers \n",
    "def remove_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "# remove urls\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "# if word starts with uppercase --> lowercase, if all chars are uppercase --> do nothing\n",
    "def lower_case(tokens):\n",
    "    tokens = [(w.lower() if not w.isupper() else w) for w in tokens]\n",
    "    return tokens\n",
    "            \n",
    "# print(lower_case(['AAAAA', 'army', 'Army'])) # test function\n",
    "# print(remove_urls(remove_punctuations(remove_numbers('740 test test 3 99ma http://cnn.com')))) # test function\n",
    "\n",
    "#remove elongated words\n",
    "def remove_elongated(text):\n",
    "    el = []\n",
    "    setofwords = set(words.words())\n",
    "    for word in text.split():\n",
    "        if word in setofwords:\n",
    "            pass\n",
    "        else:\n",
    "            word=re.sub(r'(?i)(.)\\1+', r'\\1', word)\n",
    "        el.append(word)\n",
    "    return el\n",
    "\n",
    "#test = 'Aweeeesome president Trump greeeeat good Ameerica '\n",
    "#print(remove_elongated(test))\n",
    "\n",
    "data['text'] = data.apply(lambda x: remove_numbers(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_punctuations(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_urls(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_elongated(x['text']), axis=1)\n",
    "# tokenize\n",
    "data['tokens'] = data.apply(lambda x: word_tokenize(x['text']), axis=1)\n",
    "\n",
    "# lower_case\n",
    "data['tokens'] = data.apply(lambda x: lower_case(x['tokens']), axis=1)\n",
    "            \n",
    "# remove stop words\n",
    "data['tokens'] = data.apply(lambda x: [w for w in x['tokens'] if w not in stop_words], axis=1)\n",
    "\n",
    "# stemming\n",
    "ps = PorterStemmer() \n",
    "data['tokens'] = data.apply(lambda x: [ps.stem(w) for w in x['tokens']], axis=1)\n",
    "\n",
    "# enlogated words, eg. 'aweeesome'; essentially we are searching for words that contain 3+ same [a-z] characters in a row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<re.Match object; span=(0, 10), match='Aweeeesome'>, None, None, <re.Match object; span=(0, 8), match='greeeeat'>, <re.Match object; span=(0, 8), match='Ameerica'>, <re.Match object; span=(0, 4), match='good'>, <re.Match object; span=(0, 4), match='book'>]\n"
     ]
    }
   ],
   "source": [
    "def check_elongated(text):\n",
    "    el = []\n",
    "    regex_el = r\"\\w*(\\w)\\1\\w*\"\n",
    "    for word in text.split():\n",
    "        el.append(re.search(regex_el, word))\n",
    "    return el\n",
    "    \n",
    "test = 'Aweeeesome president Trump greeeeat Ameerica good book'\n",
    "print(check_elongated(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Awesome', 'president', 'Trump', 'great', 'good', 'America']\n",
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('book', 0.29832935560859186)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
